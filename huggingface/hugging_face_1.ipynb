{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \" I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "chatbot = pipeline(\"text2text-generation\", model=\"facebook/blenderbot-400M-distill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \" I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?\"}]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "# Example usage: Input a conversation starter\n",
    "input_text = \"Hello! How are you today?\"\n",
    "response = chatbot(input_text)\n",
    "\n",
    "# Display the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Conversation' from 'transformers' (/usr/local/python/3.10.13/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conversation\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Conversation' from 'transformers' (/usr/local/python/3.10.13/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' I like snowboarding and skiing.  What do you like to do in winter?'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chatbot(\"What else do you recommend?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \" Well, I'm not sure what else I can think of, but I do know that I'm going to miss her.\"}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.add_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4d9557d1494eada70011bf30a213d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab301668f3214175ba8b805f6d75601c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721b146616434bcaa437853a061316ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8f5ab7bc1147e8b2d9bf21324fc17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d84e7fe3b0f4633ae28699d62a867b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82228ea481b4ac9830f0cf7ce8f362e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e629c7b4fab6408d80a53d2539a3ae31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"KevSun/Engessay_grading_ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohesion: 8.0\n",
      "syntax: 8.5\n",
      "vocabulary: 8.5\n",
      "phraseology: 8.5\n",
      "grammar: 9.0\n",
      "conventions: 8.5\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Kevintu/Engessay_grading_ML\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KevSun/Engessay_grading_ML\")\n",
    "\n",
    "new_text = \"Keeping in view the ongoing trend of all educational institutes, a rising number of schools are providing tablets and laptops in replacement of books, contracting the use of other printed material. Well, this latest use of technology has its own pros and cons but obviously advantages outweigh disadvantages which will be discussed further. Provision of laptops to students provides them with a myriad of benefits. First and foremost remains to be the convinience in carrying due to the latest light weighted models. There is no need for the students to carry the heavy baggages as practiced previously due to multiple subject books and copies. This also makes each and every subject to be available to them during study hours unlike before when an individual could just a certain number of books.\\\n",
    "In addition, these help the institute administration to carry out computer based examinations with the help of Artificial intelligence. Examinations are conducted under the supervision of a webcam with the help of AI database which could easily detect if a person attempts to shift the screen or do anything unethical. This in turn helps thwart the squander of  resources like  bulk of papers and stationary as well as the time as all mathematical calculations can be done within seconds.\\\n",
    "But as we are already aware, pros and cons always go also may be hand in hand. This latest technology trend has made great dependance upon the internet connection which could ruin a persons examination or any important lecture if not connected efficiently and the students are now habitual of its ease and no one now prefers the previous method.\\\n",
    "To sum up, keeping in view the above discussion, there is a plethora of benefits in replacing the printed materials and obviously one has to relinquish the olden methods in order to meander with the latest trends. \"\n",
    "encoded_input = tokenizer(new_text, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "\n",
    "predictions = outputs.logits.squeeze()\n",
    "predicted_scores = predictions.numpy()  # Convert to numpy array\n",
    "item_names = [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n",
    "\n",
    "# Scale predictions from 1 to 10 and round to the nearest 0.5\n",
    "scaled_scores = 2.25 * predicted_scores - 1.25\n",
    "rounded_scores = [round(score * 2) / 2 for score in scaled_scores]  # Round to nearest 0.5\n",
    "\n",
    "for item, score in zip(item_names, rounded_scores):\n",
    "    print(f\"{item}: {score:.1f}\")\n",
    "\n",
    "# Example output:\n",
    "# cohesion: 6.5\n",
    "# syntax: 7.0\n",
    "# vocabulary: 7.5\n",
    "# phraseology: 7.5\n",
    "# grammar: 7.5\n",
    "# conventions: 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
