{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to find latest models by most downlaoded. Find model and saving that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(id='1231czx/llama3_it_ultra_list_and_bold500', author=None, sha=None, created_at=datetime.datetime(2024, 9, 3, 12, 55, 17, tzinfo=datetime.timezone.utc), last_modified=None, private=False, disabled=None, downloads=25854145, downloads_all_time=None, gated=None, gguf=None, inference=None, likes=2, library_name='transformers', tags=['transformers', 'safetensors', 'llama', 'text-classification', 'arxiv:1910.09700', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us'], pipeline_tag='text-classification', mask_token=None, card_data=None, widget_data=None, model_index=None, config=None, transformers_info=None, trending_score=None, siblings=None, spaces=None, safetensors=None)]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class from Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Create an instance of the API\n",
    "api = HfApi()\n",
    "\n",
    "# Return the filtered list from the Hub using direct parameters\n",
    "models = api.list_models(\n",
    "    task=\"text-classification\",  # Directly specify the task\n",
    "    sort=\"downloads\",            # Sort by the number of downloads\n",
    "    direction=-1,                # Sort in descending order\n",
    "    limit=1                      # Limit the result to the top model\n",
    ")\n",
    "\n",
    "# Store as a list\n",
    "model_list = list(models)\n",
    "\n",
    "# Print the result to see the top model\n",
    "print(model_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Define the model ID\n",
    "modelId = \"1231czx/llama3_it_ultra_list_and_bold500\"\n",
    "\n",
    "# Load the model from the Hugging Face Hub\n",
    "model = AutoModel.from_pretrained(modelId)\n",
    "\n",
    "# Define the save directory\n",
    "save_directory = f\"models/{modelId.replace('/', '_')}\"\n",
    "\n",
    "# Save the model to the specified directory\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face built the dataset package for interacting with datasets. There are a lot of convenient functions, including load_dataset_builder which we just used. After inspecting a dataset to ensure its the right one for your project, it's time to load the dataset! For this, we can leverage input parameters for load_dataset to specify which parts of a dataset to load, i.e. the \"train\" dataset for English wikipedia articles.\n",
    "\n",
    "The load_dataset module from the datasets package is already loaded for you. Note: the load_dataset function was modified for the purpose of this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': Value(dtype='string', id=None), 'qid': Value(dtype='string', id=None), 'name': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataset is 1560888\n"
     ]
    }
   ],
   "source": [
    "# Load the module\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "\n",
    "# Create the dataset builder\n",
    "reviews_builder = load_dataset_builder(\"derenrich/wikidata-en-descriptions-small\")\n",
    "\n",
    "# Print the features\n",
    "print(reviews_builder.info.features) ## these are columns\n",
    "\n",
    "\n",
    "# Load the train portion of the dataset\n",
    "wikipedia = load_dataset(\"derenrich/wikidata-en-descriptions-small\", split=\"train\")\n",
    "\n",
    "print(f\"The length of the dataset is {len(wikipedia)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Produce a short Wikidata description for this Wikipedia article ### Input: The 1964–65 DDR-Oberliga was the 16th season of the DDR-Oberliga, the first tier of league football in East Germany. The league was contested by fourteen teams. National People's Army club ASK Vorwärts Berlin won the championship, the club's fourth of six national East German championships all up. Bernd Bauchspieß of BSG Chemie Leipzig was the league's top scorer with 14 goals, becoming the first player to finish as top scorer on three occasions. For the third time the title East German Footballer of the year was awarded, going to Horst Weigang of SC Leipzig. On the strength of the 1964–65 title Vorwärts qualified for the 1965–66 European Cup where the club was knocked out by Manchester United in the first round. Seventh-placed club SC Aufbau Magdeburg qualified for the 1965–66 European Cup Winners' Cup as the seasons FDGB-Pokal winner and was knocked out by West Ham United in the quarter-finals. Fourth-placed SC Leipzig qualified for the 1965–66 Inter-Cities Fairs Cup where it was knocked out in the second round by Leeds United. ### Response: sports season\n"
     ]
    }
   ],
   "source": [
    "# Filter the documents\n",
    "filtered = wikipedia.filter(lambda row: \"football\" in row[\"text\"])\n",
    "\n",
    "# Create a sample dataset\n",
    "example = filtered.select(range(1))\n",
    "\n",
    "print(example[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID: Q563628, Name: 1964–65 DDR-Oberliga, Text: Instruction: Produce a short Wikidata description for this Wikipedia article ### Input: The 1964–65 DDR-Oberliga was the 16th season of the DDR-Oberliga, the first tier of league football in East Germany. The league was contested by fourteen teams. National People's Army club ASK Vorwärts Berlin won the championship, the club's fourth of six national East German championships all up. Bernd Bauchspieß of BSG Chemie Leipzig was the league's top scorer with 14 goals, becoming the first player to finish as top scorer on three occasions. For the third time the title East German Footballer of the year was awarded, going to Horst Weigang of SC Leipzig. On the strength of the 1964–65 title Vorwärts qualified for the 1965–66 European Cup where the club was knocked out by Manchester United in the first round. Seventh-placed club SC Aufbau Magdeburg qualified for the 1965–66 European Cup Winners' Cup as the seasons FDGB-Pokal winner and was knocked out by West Ham United in the quarter-finals. Fourth-placed SC Leipzig qualified for the 1965–66 Inter-Cities Fairs Cup where it was knocked out in the second round by Leeds United. ### Response: sports season\n"
     ]
    }
   ],
   "source": [
    "# Print specific columns for all rows\n",
    "for row in filtered:\n",
    "    print(f\"QID: {row['qid']}, Name: {row['name']}, Text: {row['text']}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Sentiment Analysis Output: [{'label': 'POSITIVE', 'score': 0.9992846846580505}]\n",
      "Model-Specific Sentiment Analysis Output: [{'label': 'POSITIVE', 'score': 0.9992846846580505}]\n"
     ]
    }
   ],
   "source": [
    "# Import the pipeline function from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment-analysis pipeline using the default model\n",
    "task_pipeline = pipeline(task=\"sentiment-analysis\")\n",
    "\n",
    "# Create a sentiment-analysis pipeline using a specific pre-trained model\n",
    "model_pipeline = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"I love using the Hugging Face library!\"\n",
    "\n",
    "# Predict the sentiment using the default sentiment-analysis pipeline\n",
    "task_output = task_pipeline(input_text)\n",
    "\n",
    "# Predict the sentiment using the specific model pipeline\n",
    "model_output = model_pipeline(input_text)\n",
    "\n",
    "# Print the outputs\n",
    "print(f\"Default Sentiment Analysis Output: {task_output}\")\n",
    "print(f\"Model-Specific Sentiment Analysis Output: {model_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1c4b87ce8d41588a8b9d519ea4cb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b318ad9731140dfbb23947ec33fd8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f63930a29949f382d00b1c83af94e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb99f297c7e410abc7228f4f59b13a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'ORG', 'score': np.float32(0.994665), 'word': 'Hugging Face Inc', 'start': 0, 'end': 16}, {'entity_group': 'LOC', 'score': np.float32(0.9983915), 'word': 'New York', 'start': 40, 'end': 48}]\n"
     ]
    }
   ],
   "source": [
    "# Create a named entity recognition pipeline\n",
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "# Analyze named entities in a sentence\n",
    "result = ner_pipeline(\"Hugging Face Inc. is a company based in New York.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment using AutoClasses: POSITIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries from transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Download the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Create the sentiment-analysis pipeline using the specified model and tokenizer\n",
    "sentiment_analysis = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"I love using the Hugging Face library!\"\n",
    "\n",
    "# Predict the sentiment\n",
    "output = sentiment_analysis(input_text)\n",
    "\n",
    "# Print the sentiment label\n",
    "print(f\"Sentiment using AutoClasses: {output[0]['label']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you???\n"
     ]
    }
   ],
   "source": [
    "# Import the AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Normalize the input string\n",
    "output = tokenizer.backend_tokenizer.normalizer.normalize_str(\"How are you???\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38e04d97b1644b19b3e9d32fc4cd824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac42ebeeccd408bb8aa96af3b0c2770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfa8e5919414d8f8aa2488eb14adcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618d34554ba546c997d4f2dade084b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3ca8451afb4b61acb416b61b3ea55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT tokenizer: ['P', 'ine', 'apple', 'Ġon', 'Ġpizza', 'Ġis', 'Ġpretty', 'Ġgood', ',', 'ĠI', 'Ġguess', '.']\n",
      "DistilBERT tokenizer: ['pine', '##apple', 'on', 'pizza', 'is', 'pretty', 'good', ',', 'i', 'guess', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes from the transformers library\n",
    "from transformers import GPT2Tokenizer, DistilBertTokenizer\n",
    "\n",
    "# Download the GPT tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define your input text\n",
    "input_text = \"Pineapple on pizza is pretty good, I guess.\"\n",
    "\n",
    "# Tokenize the input using GPT-2 tokenizer\n",
    "gpt_tokens = gpt_tokenizer.tokenize(text=input_text)\n",
    "\n",
    "# Repeat for DistilBERT\n",
    "distil_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "distil_tokens = distil_tokenizer.tokenize(text=input_text)\n",
    "\n",
    "# Compare the output\n",
    "print(f\"GPT tokenizer: {gpt_tokens}\")\n",
    "print(f\"DistilBERT tokenizer: {distil_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pipeline function from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text classification pipeline using the specified model\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\", \n",
    "    model=\"abdulmatinomotoso/English_Grammar_Checker\"\n",
    ")\n",
    "\n",
    "# Predict the classification for the input text\n",
    "output = classifier(\"I will walk dog\")\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91911ec0ab2468499dd7c15a4cf91de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d7de8936048f596ea32d2b802846f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c0fa6324dd478b81680519456ded15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e619ce453d54d1dbf12efd79ff16efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715ced5a93b2422aa43c6e86b6c42e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a589188f6de40bda89af2044ab810f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Label: politics with score: 0.7835\n"
     ]
    }
   ],
   "source": [
    "# Import the pipeline function from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the input text\n",
    "text = \"The government is planning to introduce new regulations to control the spread of misinformation.\"\n",
    "\n",
    "# Build the zero-shot classifier\n",
    "classifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Create the list of candidate labels\n",
    "candidate_labels = [\"politics\", \"science\", \"sports\"]\n",
    "\n",
    "# Predict the output\n",
    "output = classifier(text, candidate_labels)\n",
    "\n",
    "# Print the top label and its score\n",
    "print(f\"Top Label: {output['labels'][0]} with score: {output['scores'][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd614885720449179e29d060f2a8f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1339e2bc348b44bf8ab84b2ef9fa22f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d927ad51c3de45e999c6dec6f7a1b39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cedf63aead4a0ba6fed0e350673bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fd91b164024b549a1899cc6ca069db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456c4823a8d94bc1b79f6f80ca5c725a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Your max_length is set to 200, but your input_length is only 113. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 475\n",
      "Summary length: 304\n",
      "Summary: the solar system consists of the Sun and the objects that orbit it, including eight planets, their moons, and other non-stellar objects. The Sun is the central object, providing light and heat necessary for life on Earth. The planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "# Import the pipeline function from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the text to be summarized\n",
    "original_text = \"\"\"\n",
    "The solar system consists of the Sun and the objects that orbit it, including eight planets, their moons, and other non-stellar objects. The Sun is the central object, providing light and heat necessary for life on Earth. The planets, in order of their distance from the Sun, are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and moons, with Jupiter and Saturn being the largest and having the most moons.\n",
    "\"\"\"\n",
    "\n",
    "# Create the summarization pipeline\n",
    "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
    "\n",
    "# Summarize the text\n",
    "summary_text = summarizer(original_text)\n",
    "\n",
    "# Compare the length of the original and summarized texts\n",
    "print(f\"Original text length: {len(original_text)}\")\n",
    "print(f\"Summary length: {len(summary_text[0]['summary_text'])}\")\n",
    "\n",
    "# Print the summarized text\n",
    "print(f\"Summary: {summary_text[0]['summary_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Summary:\n",
      "the solar system consists of the Sun and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 113. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Long Summary:\n",
      "the solar system consists of the Sun and the objects that orbit it, including eight planets, their moons, and other non-stellar objects. The Sun is the central object, providing light and heat necessary for life on Earth. The planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n"
     ]
    }
   ],
   "source": [
    "# Import the pipeline function from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the original text to be summarized\n",
    "original_text = \"\"\"\n",
    "The solar system consists of the Sun and the objects that orbit it, including eight planets, their moons, and other non-stellar objects. \n",
    "The Sun is the central object, providing light and heat necessary for life on Earth. \n",
    "The planets, in order of their distance from the Sun, are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. \n",
    "Each planet has its own unique characteristics and moons, with Jupiter and Saturn being the largest and having the most moons.\n",
    "\"\"\"\n",
    "\n",
    "# Create a short summarizer with a maximum length of 10 tokens\n",
    "short_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=1, max_length=10)\n",
    "\n",
    "# Generate a short summary\n",
    "short_summary_text = short_summarizer(original_text)\n",
    "\n",
    "# Print the short summary\n",
    "print(\"Short Summary:\")\n",
    "print(short_summary_text[0][\"summary_text\"])\n",
    "\n",
    "# Create a long summarizer with a length between 50 and 150 tokens\n",
    "long_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=50, max_length=150)\n",
    "\n",
    "# Generate a long summary\n",
    "long_summary_text = long_summarizer(original_text)\n",
    "\n",
    "# Print the long summary\n",
    "print(\"\\nLong Summary:\")\n",
    "print(long_summary_text[0][\"summary_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
